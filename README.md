---

# ğŸ§  Cryptocurrency Forecasting with Deep Learning and Machine Learning

This project forecasts short-term Bitcoin price movements using engineered blockchain indicators and state-of-the-art modeling techniques:

- ğŸ” **LSTM** â€“ Memory-based sequential modeling
- ğŸ§  **Transformer** â€“ Attention-based deep learning for temporal data
- âš¡ **XGBoost** â€“ High-performance gradient boosting as a baseline

The models are trained with temporal data, extensive preprocessing, and hyperparameter tuning using Ray Tune.

---

## ğŸ“¦ Quickstart

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/mysticnova-dev/CryptoTimeSeriesAI.git
cd CryptoTimeSeriesAI
```

### 2ï¸âƒ£ Install Dependencies

We recommend using a fresh virtual environment:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run End-to-End Pipeline (Transformer)

```bash
python run_transformer_pipeline.py
```

This script will:

- Load the dataset
- Clean and engineer features
- Train + tune the Transformer model
- Visualize losses and predictions
- Save all artifacts (metrics, plots, models)

### 4ï¸âƒ£ Run LSTM or XGBoost Baselines

```bash
python run_lstm_pipeline.py
python run_xgboost_pipeline.py
```

> ğŸ“‚ All outputs are saved under `./LSTM/Models/`, `./Transformer/Models/`, or `./XGBoost/`.

---

## ğŸ“Š Dataset

The dataset consists of daily Bitcoin blockchain metrics including price, supply, volume, mining difficulty, and transactional activity. See the full [data dictionary](#ğŸ“Š-dataset-description) below.

Ensure the dataset is named `bitcoin_dataset.csv` and placed in the root directory or update the path in the `run_*.py` scripts.

---

## ğŸ› ï¸ Project Structure

```
â”œâ”€â”€ bitcoin_dataset.csv              # Input CSV file (daily blockchain metrics)
â”œâ”€â”€ run_transformer_pipeline.py     # Full Transformer training pipeline
â”œâ”€â”€ run_lstm_pipeline.py            # LSTM training script
â”œâ”€â”€ run_xgboost_pipeline.py         # XGBoost regression baseline
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_cleaning_utils.py      # Handles missing values
â”‚   â””â”€â”€ generate_btc_features.py    # Feature engineering logic
â”œâ”€â”€ Transformer/
â”‚   â””â”€â”€ Models/                     # Saved Transformer models, plots, and metrics
â”œâ”€â”€ LSTM/
â”‚   â””â”€â”€ Models/                     # Saved LSTM models, plots, and metrics
â”œâ”€â”€ XGBoost_regressor_demo.pdf      # XGBoost demonstration
â”œâ”€â”€ Transformer_Demo.pdf            # Transformer explanation and visuals
â”œâ”€â”€ LSTM_Demo.pdf                   # LSTM explanation and visuals
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Dataset Description

| Feature | Description |
|--------|-------------|
| `Date` | Date of observation |
| `btc_market_price` | Average USD market price across major exchanges |
| `btc_total_bitcoins` | Total bitcoins mined |
| `btc_market_cap` | USD market cap of circulating supply |
| `btc_trade_volume` | USD value traded daily |
| `btc_hash_rate`, `btc_difficulty` | Mining difficulty and hash rate |
| `btc_miners_revenue`, `btc_transaction_fees` | Miner rewards & transaction fees |
| `btc_cost_per_transaction` | Cost per TX for miners |
| `btc_n_transactions`, `btc_n_unique_addresses` | On-chain activity metrics |
| `btc_output_volume`, `btc_estimated_transaction_volume` | Total value of outputs |

See code for full feature usage and transformations.

---

## âœ¨ Feature Engineering

Features are generated using domain-relevant indicators:

- **Volatility Metrics:** Bollinger Bands, ATR
- **Momentum Indicators:** ROC, Momentum
- **Volume Patterns:** OBV, RVOL, VPT
- **Supply-Demand Ratios:** Market cap / supply
- **Log-transformed Variables**
- **Missingness Flags**

---

## ğŸ§  Model Overview

### ğŸ” LSTM

- Recurrent neural network with memory cells
- Tunable sequence length and hidden layers
- Sliding-window sequence generation
- Dropout + learning rate scheduling
- Early stopping for generalization

### ğŸ§  Transformer

- Positional encoding + self-attention
- Multi-head architecture with MLP decoder
- Flattened token sequence â†’ global prediction
- Ray Tune + HyperOpt search space
- Full visualization and model checkpointing

### âš¡ XGBoost

- High-performance gradient-boosted trees
- Tabular feature format (no sequence modeling)
- Robust to noise and fast to train
- Serves as a predictive baseline

---

## ğŸ” Hyperparameter Tuning

Uses Ray Tune with `ASHAScheduler` and `HyperOptSearch`:

- 100 trial budget with early stopping
- Parallel trials with CPU/GPU resource control
- Automatic trial logging and saving
- Best model auto-retrained and exported

---

## ğŸ“ˆ Evaluation & Visualization

Each model is evaluated using:

| Metric | Meaning |
|--------|---------|
| `MSE` | Mean Squared Error |
| `RMSE` | Root Mean Squared Error |
| `MAE` | Mean Absolute Error |
| `RÂ²` | Coefficient of determination |

You also get:

- ğŸ“‰ Training vs Validation Loss
- ğŸ“ˆ Actual vs Forecasted Prices
- ğŸ” Zoomed Forecasts
- ğŸ“Š Residuals Plot
- ğŸ“„ Metric + Config Reports

---

## ğŸ”¬ Best Transformer Results (Tuned)

| Metric | Value |
|--------|-------|
| MSE    | 378,066.09 |
| RMSE   | 614.87 |
| MAE    | 271.34 |
| RÂ²     | 0.7616 |

Best Hyperparameters:

```yaml
seq_len: 8
batch_size: 4
lr: 5.733e-05
patience: 400
min_delta: 0.0001
nhead: 4
num_layers: 8
dim_feedforward: 128
dropout: 0.211
num_mlp_units: 128
```

---

## âœ… Requirements

Install all dependencies with:

```bash
pip install -r requirements.txt
```

Minimum required packages:

```
numpy
pandas
scikit-learn
matplotlib
torch
xgboost
ray[default]
hyperopt
```

---




## ğŸš€ Future Work

- ğŸ” *Hypercomplex Neural Net Forecasting: Extend the architecture using quaternion, octonion, and even sedenion-based neural networks to capture entangled market signals across multi-dimensional manifolds, enabling richer representation learning for temporal financial data.
- ğŸ§  **Bayesian Smoothing**: Apply Bayesian inference for uncertainty-aware predictions and to regularize noisy outputs in volatile market zones.
- ğŸ“‰ **Residual Modeling**: Predict model residuals as a secondary signal and recursively refine the primary forecasts using residual correction loops.
- ğŸ§® **Hierarchical Forecasting**: Integrate taxonomic structures (e.g., price â†’ trend + volatility components) with specialized submodels.
- ğŸ“ˆ **Feature Importance from Attention Maps**: Extract interpretability insights by aggregating self-attention weights from transformer heads.


---

## ğŸ§  Author

**Leonard Burtenshaw**  
AI Engineer | Forecast Architect | Data Science Specialist  
[LinkedIn](https://linkedin.com/in/leoharrisai/)  

---


